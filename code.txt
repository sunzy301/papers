1.caffe cosine
修改caffe中 sgd_solver.cpp
else if (lr_policy == "cosine") {
    rate = (this->param_.base_lr() - this->param_.min_lr()) * Dtype(0.5) * 
         (cos(Dtype(3.141592653) * (this->iter_ % this->param_.stepsize()) / this->param_.stepsize())
         + Dtype(1.0)) + this->param_.min_lr();
  }
在caffe.proto中添加solver parameter
// for cosine learning rate policy
  optional int32 Titer = 51;
  optional float min_lr = 50 [default = 0.0];
cosine方法对于分类网络训练效果有普适性提高

############################################################
2.ssd
2.1 安装编译
安装流程
# Modify Makefile.config according to your Caffe installation.
cp Makefile.config.example Makefile.config
make -j8
# Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.
make py
make test -j8
# (Optional)
make runtest -j8

参考 <https://github.com/weiliu89/caffe/tree/ssd> 
总体基本和caffe原版一致


运行make之后出现如下错误：
/usr/include/boost/property_tree/detail/json_parser_read.hpp:257:264: error: ‘type name’ declared as function returning an array 
escape 
^ 
/usr/include/boost/property_tree/detail/json_parser_read.hpp:257:264: error: ‘type name’ declared as function returning an array 
make: * [.build_release/cuda/src/caffe/layers/detection_output_layer.o] Error 1 
make: * Waiting for unfinished jobs….
解决办法： 
修改json_parser_read.hpp：打开文件夹Document，选中computer，在搜索json_parser_read.hpp，找到该文件的路径之后用如下命令打开
sudo gedit /usr/include/boost/property_tree/detail/json_parser_read.hpp
将257行开始的escape代码段注释掉即可，如下：
/*escape
                    =   chset_p(detail::widen<Ch>("\"\\/bfnrt").c_str())
                            [typename Context::a_escape(self.c)]
                    |   'u' >> uint_parser<unsigned long, 16, 4, 4>()
                            [typename Context::a_unicode(self.c)]
                    ;*/

参考 <https://blog.csdn.net/u013832707/article/details/53426923> 
应该升级gcc到5.x版本，但是这样改动比较大，直接修改也可以

2.2 数据读入
2.2.1 准备数据集
Voc2007&2012
下载三个数据集
# Download the data.
cd $HOME/data
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar
# Extract the data.
tar -xvf VOCtrainval_11-May-2012.tar
tar -xvf VOCtrainval_06-Nov-2007.tar
tar -xvf VOCtest_06-Nov-2007.tar

2.2.2 创建lmdb文件
cd $CAFFE_ROOT
# Create the trainval.txt, test.txt, and test_name_size.txt in data/VOC0712/
创建训练验证集与测试集的图像标记列表
并且为混淆训练验证集顺序，记录测试集的图像大小
./data/VOC0712/create_list.sh
# You can modify the parameters in create_data.sh if needed.
# It will create lmdb files for trainval and test with encoded original image:
#   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb
#   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb
# and make soft links at examples/VOC0712/
创建lmdb
./data/VOC0712/create_data.sh

该脚本会调用scripts/create_annoset.py进行变量的检查
最终会使用build/tools/convert_annoset工具进行lmdb打包
核心的读入函数为ReadRichImageToAnnotatedDatum
读入的图像可能使用jpg编码存储在lmdb中以节约空间

标记数据存储结构
一级结构AnnotationDatum
包含Datum存储图像数据，以及多个AnnotationGroup，每个代表一个类别
一个AnnontationGroup有一个类别，对应多个Annotation AnnotationGroup
每个Annotation对应一个框，包含在Group中编号和一个位置BBOX Annotation
位置BBOX NormalizedBBox

2.3 模型结构
2.3.1 生成模型结构
创建prototxt文件并且进行运行

# It will create model definition files and save snapshot models in:
#   - $CAFFE_ROOT/models/VGGNet/VOC0712/SSD_300x300/
# and job file, log file, and the python script in:
#   - $CAFFE_ROOT/jobs/VGGNet/VOC0712/SSD_300x300/
# and save temporary evaluation results in:
#   - $HOME/data/VOCdevkit/results/VOC2007/SSD_300x300/
# It should reach 77.* mAP at 120k iterations.
python examples/ssd/ssd_pascal.py

具体结构参考文档
https://blog.csdn.net/u014380165/article/details/72824889
对于每个特征层
都会有一个priorbox层，生成默认的box位置，这个不参与梯度回传
然后会有两个head，分别预测位置回归和类别分类
每个head由一个卷积层，一个permute层，一个flat层组成，后面两个层主要是变换shape
最后由concat层合并各个层的head，送到loss计算或者输出检测结果


2.3.2 SSD 检测head
1)每个特征层输出是
[batch_size, channel, height_width]
其中channel是特征层的维度

2)然后经过一个卷积层，输出是
[batch_size, channel, height_width]
其中channel和head的类别有关系，一般是num_priorbox*num_class或者num_priorbox*4

3)经过permute交换维度，输出是
[batch_size, height, width, channel]
permute是caffe-ssd添加的层

4)flatten拉成向量,输出是
[batch_size, height*width*channel]
flatten是caffe原版自带的层

5)合并多个特征层的输出
[batch_size, sum_channel]
分类和回归会分别合并一个特征输出，进行后续的结果输出或者损失计算
concat是caffe原版自带的层

2.4 新增关键层
2.4.1 数据读入层
读入数据预处理的顺序是
1)distort, expand
2)batch sample
3)transform（mirror, scale, crop）

从lmdb读出数据时，需要对于gt bbox的shape进行特殊编排
每个样本的bbox数量不完全一致
把每个bbox的信息存储成一个8维的向量

2.4.2 permuate layer
交换输入blob的channel
count是blob所有元素的数量，对齐进行遍历
然后根据新旧order顺序计算新的位置
steps是不同维度，之前所有维度长度的乘积

############################################################
3.py faster rcnn
3.1 安装流程
Installation (sufficient for the demo)
	1. Clone the Faster R-CNN repository
# Make sure to clone with --recursive
git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git
	1. We'll call the directory that you cloned Faster R-CNN into FRCN_ROOT
Ignore notes 1 and 2 if you followed step 1 above.
Note 1: If you didn't clone Faster R-CNN with the --recursive flag, then you'll need to manually clone the caffe-fast-rcnn submodule:
git submodule update --init --recursive
Note 2: The caffe-fast-rcnn submodule needs to be on the faster-rcnn branch (or equivalent detached state). This will happen automatically if you followed step 1 instructions.
	2. Build the Cython modules
cd $FRCN_ROOT/lib
make
	3. Build Caffe and pycaffe
cd $FRCN_ROOT/caffe-fast-rcnn
# Now follow the Caffe installation instructions here:
#   http://caffe.berkeleyvision.org/installation.html

# If you're experienced with Caffe and have all of the requirements installed
# and your Makefile.config in place, then simply do:
make -j8 && make pycaffe
	4. Download pre-computed Faster R-CNN detectors
cd $FRCN_ROOT
./data/scripts/fetch_faster_rcnn_models.sh
This will populate the $FRCN_ROOT/data folder with faster_rcnn_models. See data/README.md for details. These models were trained on VOC 2007 trainval.

参考 <https://github.com/rbgirshick/py-faster-rcnn#installation-sufficient-for-the-demo> 

faster-rcnn文件夹下make
编译目标是utils.cython_bbpx, nms.cpu_nms, nms.gpu_nms, pycocotools._mask
其余相对于fast-rcnn新增的层是python编写的

3.1.1 安装问题
1)-R gcc参数不支持
修改python中distutils/unixcompiler.py,
将-R改为-Wl,rpath=

2)cudnn支持版本过低
需要将fast-rcnn中内容合并到最新的caffe版本中，可以使用git合并
可以参考：http://www.voidcn.com/article/p-nzbjzafl-cc.html

由于无法链接github，使用beyondcompare进行手动合并
将所有fast-rcnn中新增文件复制到caffe-master文件夹中，并且需要手动合并caffe.proto和blocking_queue.cpp中的修改内容
修改python_layer.hp
版本合并后，打开caffe-fast-rcnn/include/caffe/layers/python_layer.hp，self_.attr("phase") = static_cast<int>(this->phase_);这一行(line29)删除 

3.1.2 运行demo
./tools/demo.py

1)遇到如下问题
has no attribute 'param_str_'
将proposal_layer.py文件中param_str_修改为param_str

2.遇到如下问题
has no attribute 'phase_'
主要原因是python文件相对于caffe版本比较古老，已经有了变化
在config.py中添加一个phase项
__C.PHASE='TEST'
然后将proposal_layer.py中的内容进行修改
cfg_key = cfg.PHASE

demo是faster rcnn的测试过程
测试中boxes提取RPN生成的RoIs

RoIs是[n,4]维度的二维矩阵，n是roi的数量

然后读取位置回归的数据，会对于每个类别有个回归的微调值
回归微调值box_deltas是[n,21*4]维度的二维数据，n是roi的数量
每个box加上微调的delta值，也变成[n,21*4]维度的二维数据pred_boxes
加上roi的类别分数矩阵scores，其维度是[n,21]

3.2 模型结构
标准faster rcnn使用VGG作为基础结构，主要是添加了RPN结构，以及增加ROIPooling和之后的检测端
从VGG的5_3卷积出提取特征，经过rpn卷积层
设置每个位置应该对应于9个anchor
左边通路产生36维特征图，对应于4*9，每个anchor有4维位置
右边通路产生18维特征图，对应于2*9，每个anchor进行softmax分类，是物体还是背景
最后合并两个维度输入，产生roi，这个层是自定义的proposal_layer

训练时，需要使用rpn-data从gt中进行box的对应，使用anchor_target_layer层
然后基于rpn前后景分类和位置信息回归计算损失函数，分别是rpn_loss_cls和rpn_loss_bbox


3.3 新增关键层
1)proposal_layer.py
生成anchor，输入是rpn计算得到的anchor类别和delta微调
每个cell都会共享默认anchor，然后基于cell的位置计算具体anchor位置
之后需要加上rpn生成的微调值
最后基于rpn对于前后景分类的分数进行nms，减少anchor数目

2)proposal_target_layer.py
输入是rpn生成的roi和gt boxes
为每个roi设置分类以及回归的目标
计算roi和gt boxes的重叠系数，然后将重叠系数比较大的设为正样本，中间范围的设为负样本
正负样本组合送特征提取和之后的训练
只有正样本才会设置微调的目标

3)Anchor_target_layer.py
将rpn生成的anchor对应到gt box上
产生rpn对于anchor前后景分类以及位置微调的目标

基本算法：
在每个位置计算一组anchor
筛选在图像位置内的anchor
计算每个保留anchor和gtboxes的重叠系数
设置每个保留anchor的前后景label以及位置微调回归目标值

4)roi_data_layer
数据读入层
输出图像data，图像长宽信息im_info，以及标记信息gt_boxes

还有两个简单cpp实现
5)smooth_L1_loss用于位置微调的回归损失
实现上可能存在weight_in和weight_out两个调整值
而且具体还可以有一个sigma2调整smooth的具体计算方式

6)ROIPooling层
代码中硬编码使用maxpooling
roi编码是[batch_index,x1,y1,x2,y2]
其中batch_index是代表该roi对应batch中哪张图像
之后的位置编码是基于原图的，所以需要设置spatial_scale来转换到feature map上的位置

############################################################
4.refine det
4.1 使用流程
4.1.1 安装流程
make all -j16

编译Cython部分模块
cd $RefineDet_ROOT/test/lib
make -j 4
Cython部分模块代码和和py-faster-caffe类似，可能会遇到相似问题，如果遇到一一解决即可
其中包含util中bbox计算IoU部分代码，NMS的cpu与gpu实现，以及pycoco代码

4.1.2 数据模型准备
模型下载
由于外网链接先只下载vgg
https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6
这个vgg是全连接版本，使用卷积层代替fc层

数据准备
根据任务VOC0712，对于VOC2007进行测试
数据格式，以及相关打包和测试与Caffe-SSD一致

4.1.3 模型训练
python example/refinedet/VGG16_VOC2007_320.py
该训练脚本自动生成训练，测试，部署的prototxt，并可直接进行训练
脚本编写风格类似于py-faster-rcnn

训练是需要保证RefineDet的pycaffe版本是PYTHONPATH中的优先变量


4.2 模型结构
预测时使用DetectionOutput层
输入包含odm以及arm的输出其中arm_priorbox是default anchor层

4.2.1 TCB结构
TCB Block结构
类似于FPN的结构
P就是ODM的特征层编号
PX-up是从高层特征层到低层的倒卷积层
TLX_1是第一个卷积层
TLX_2是第二个卷积层，其激活层在eltwise层之后完成

4.2.2 ARM结构
ARM和SSD类似
每个特征层输出三个，分别是前后景分类，位置微调回归，以及priorbox生成
前后景分类结构如下
位置微调回归
priorbox生成
最后合并多个特征层的前后景分类，位置微调回归，以及priorbox生成
使用concat层实现
训练的时候需要基于各个特征层的输出项，计算arm的loss

4.2.3 ODM结构
ODM和SSD类似
每个特征层输出anchor的细致分类与位置回归微调
细致分类
位置回归微调
最后合并多个特征层的细致分类和位置微调回归
使用concat层实现
训练的时候需要基于各个特征层的输出项，计算odm的loss

4.2.4 head结构
head与ssd的head一致
1)每个特征层输出是
[batch_size, channel, height_width]
其中channel是特征层的维度

2)然后经过一个卷积层，输出是
[batch_size, channel, height_width]
其中channel和head的类别有关系，一般是num_priorbox*num_class或者num_priorbox*4

3)经过permute交换维度，输出是
[batch_size, height, width, channel]
permute是caffe-ssd添加的层

4)flatten拉成向量,输出是
[batch_size, height*width*channel]
flatten是caffe原版自带的层

5)合并多个特征层的输出
[batch_size, sum_channel]
分类和回归会分别合并一个特征输出，进行后续的结果输出或者损失计算
concat是caffe原版自带的层


4.3 新增关键层
4.3.1 annotated_data_layer
数据读入层的prototxt定义主要包含如下几个参数
data_param是基础的batchsize以及lmdb位置
transform_param是数据增强参数
annotated_data_param包含batch_sample以及label_map_file

DataLayerSetUp函数读入annotated_data_layer的预设参数，并且reshape输出blob大小
从lmdb读出数据时，需要对于gt bbox的shape进行特殊编排
每个样本的bbox数量不完全一致
把每个bbox的信息存储成一个8维的向量
读入数据预处理的顺序是
1)distort， expand
2)batch sample
3)transform（mirror，scale，crop）
bbox信息会跟着预处理改变，以保证匹配实际图像

4.3.2 permute_layer
交换输入channel

count是blob所有元素的数量，对齐进行遍历
然后根据新旧order顺序计算新的位置
steps是不同维度，之前所有维度长度的乘积

4.3.3 detection_output_layer
输入包含arm和odm的合并输出层

RefineDet的arm和loc预测输出都是共享类别的，所有类别产生一组位置微调delta
这个类似于SSD，和Faster-Rcnn不一致

其中后处理根据每个类别进行NMS
然后对于每张图像，设置一个最大keep_top_k，需要根据score进行排序，挑选概率最大的检测结果
输出是一个[1,1,N,7]的blob
其中N是该batch所有的检测结果的总数，可能包含了不同图像的结果
image_id指定在这个里面了

该层还可以存储检测结果
存储于xml或json格式（取决于任务类型VOC or COCO），以及在图像上的输出

4.3.4 detection_evaluate_layer
层的输入就是detection_out和gt_label

输出维度是[1,1,N,5]
其中N是检测数量+类别数量
5维分别代表

对于类别数量部分，记录的是gt中实际类别的数量
对于检测数量部分，记录检测是正样本还是负样本，使用true_pos和false_pos记录

判断正样本和负样本逻辑：
固定一张图像和一个类别，对于多个检测结果进行conf排序，然后从高到低匹配
如果一个检测和一个gtbox的IoU大于0.5，则两者匹配，认为是true_pos；之后的检测再与这个gtbox匹配就是重复匹配，认为是false_pos

4.3.5 normalize_layer
基础功能是将特征层按照单个cell的不同维度，归一化到[0,1]范围内
然后根据预设的超参数乘以scale系数

4.3.6 smooth_L1_loss_layer
使用fast-rcnn提出的smooth L1 loss用于位置微调回归

4.3.7 multibox_loss_layer
基本复用了SSD的代码
训练阶段有两个multibox，ARM和ODM各有一个层输入

Odm阶段不会直接把loss的梯度回传给arm层
在multibox中分别计算loc和conf的loss，然后加在一起
需要考虑odm阶段中还有arm的loc和conf
训练时需要为每个输出的box匹配gt以计算损失
具体匹配策略：match_type 在作者的代码中设置为PER_PREDICTION，即对每个 gnd box，选择最佳      匹配的prior box作为positive box；以及对每个prior box，如果与其覆盖率最高的gnd box的覆盖率超过overlap_threshold，也将其作为positive box

MineHardExamples中实现基于arm的筛选和最终的训练样本筛选
以保证正负样本比例合适
 在进行hard neg mining时，按如下步骤进行计算：
    - 计算每个prior box的confidence loss。
    - 如果mining_type为HARD_EXAMPLE，计算location loss，并将location loss和conf
      loss加起来作为损失。注意prior box和gnd box为(xmin, ymin, xmax, ymax)格式，
      但location pred为(ncx, ncy, nw, nh)格式，其中ncx，ncy为中心点偏移，用prior
      box宽高和variance[0]和 variance[1]归一化。nw和nh为宽度和高度偏移，用prior 
      box宽高和variance[2]和variance[3]归一化。需要依次调用EncodeLocPrediction和
      ComputeLocLoss计算位置损失。
    - 如果has_nms_param为true，则对negative window进行nms后再选取。
      如果use_prior_for_nms，则使用prior box进行nms，否则使用pred box进行nms。在
      进行nms之前需要先调用DecodeBBoxes将pred bbox转化为(xmin, ymin, xmax, ymax)
      格式，然后再调用ApplyNMS。
    - 如果mining_type为MAX_NEGATIVE，则根据neg_pos_ratio以及positive数目确定选取
      的negative数目，否则根据sample_size确定要选取的negative数目


4.3.8 prior_box_layer
该层用于生成初始的anchor
层输入和设置，其中min_size就是box的长度，aspect_ratio是box的比例
variance表示长和宽对于loss贡献不同，以进行归一化
输出维度为 [1,2,H*W*num_prior*4]
初始default anchor数量计算公式
Num_prior = aspect_size.size() * min_size.size()
If max_size:
	Num_prior++
	

辅助文件
1)bbox_util
包含各种对于bbox的处理函数
包括nms，难例挖掘，检测结果和gt的匹配等
具体可见bbox_util.hpp

2)im_transform
图像预处理函数
主要难点是检测框需要随着预处理变动，保证和图像能够对应

3)sampler
Batch sampler采样，用于数据增强
