CNN网络架构
介绍各个网络结构与基础技术
1 传统堆叠式结构
1.1 Lenet
1.2 Alexnet
论文名称：ImageNet Classification with Deep Convolutional Neural Networks
论文地址：https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf
论文简要笔记:
  首次使用CNN神经网络，在ImageNet分类任务上大幅度超过传统方法
参考：
  https://blog.csdn.net/zyqdragon/article/details/72353420

1.3 VGGnet
论文名称：VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION
论文地址：1409.1556v6
论文简要笔记:
  VGG有A-E五种深浅不一的配置。
  主要其多使用3*3的小卷积来代替Alexnet的大卷积。
  并且提出一个CNN设计原则，feature map减半，则维度翻倍
  A-E不同深度，如果需要训练深的网络，则先训练浅层网络，然后将浅层网络作为深层网络的初始值。
参考：
  https://blog.csdn.net/muyiyushan/article/details/62895202

2 Inception
2.1 Inception v1
论文名称：Going deeper with convolutions
论文地址：1409.4842
论文简要笔记:
  提出Inception Block结构
  首先提出使用多分支卷积然后合并的Block结构，并且配合中间层监督达到很好的分类效果
  使用1*1卷积降低维度，减少计算量
参考：
  https://www.cnblogs.com/yinheyi/p/6930799.html

2.2 Inception v2
论文名称：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
论文地址：1502.03167
论文简要笔记:
  提出Batch Nomalization层
  其在训练的时候，根据一个mini-batch，将神经网络输出映射到[0,1]中
  BN层是非常重要的CNN设计思想，能够大幅度提高模型收敛能力，防止梯度的弥散和爆炸
参考：
  https://blog.csdn.net/linmingan/article/details/50780761
  https://blog.csdn.net/malefactor/article/details/51476961

2.3 Inception v3
论文名称：Rethinking the Inception Architecture for Computer Vision
论文地址：1512.00567v3
论文简要笔记:
  去除所有大于3*3的卷积
  可以使用多个3*3卷卷积叠加使用达到近似效果，或者使用1*k和k*1卷积的叠加使用
参考：
  https://blog.csdn.net/kangroger/article/details/69218625

2.4 Inception v4
论文名称：Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning
论文地址：1602.07261v1
论文简要笔记:
  提出Resnet和Inception混合的Block，可以结合两个模型的优势
  又有多分支采样和卷积，又有残差链接
参考：
  https://blog.csdn.net/wspba/article/details/68951488?utm_source=itdadao&utm_medium=referral

2.5 Xception
论文名称：Xception: Deep Learning with Depthwise Separable Convolutions
论文地址：1610.02357v3
论文简要笔记:
  Xception是google继Inception后提出的对Inception v3的另一种改进，
  主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作
  depthwise卷积可以大幅度降低计算量
参考：
  https://blog.csdn.net/u014380165/article/details/75142710

3 Resnet
3.1 Resnet
论文名称：Deep Residual Learning for Image Recognition
论文地址：1512.03385
论文简要笔记:
  提出重要的残差卷积结构
  可以更好地从低层跳接传输特征到上层，同样也可以帮助梯度回传，加速收敛
  Res Block一般以瓶颈结构设计，使用1*1卷积降维，然后使用3*3卷积提取特征，最后再用1*1卷积升维，以减少计算量
  首次将模型层数增加到100层以上
参考：
  https://blog.csdn.net/wspba/article/details/57074389

3.2 RexNeXt
论文名称：Aggregated Residual Transformations for Deep Neural Networks
论文地址：1611.05431v2
论文简要笔记:
  使用多个相同结构的通路进行计算，在每个通路中降低维度
  实际操作中使用group conv
  相较于普通的ResNet，可以在保持参数量和计算量的同时提高准确度
参考：
  https://blog.csdn.net/u014380165/article/details/71667916

4 Densenet
4.1 Densenet
论文名称：Densely Connected Convolutional Networks
论文地址：1608.06993v3
论文简要笔记:
  创造了DenseNet
  让每个层都连接之后所有的层，并且接受之前所有层的输入，形成一个独立的块
  不同块之间用pooling与卷积调整大小
  每个层的维度比较小，一般只有12
  整个网络更加好训练，更加节约参数量，也能取得更好的分类效果
参考：
  https://blog.csdn.net/u012938704/article/details/53468483
  https://blog.csdn.net/u014380165/article/details/75142664

4.2 ConDensenet
论文名称：CondenseNet: An Efficient DenseNet using Learned Group Convolutions
论文地址：1711.09224v1
论文简要笔记:
  创新点：让group是可学习的，但是通过index，在test过程中可以直接使用group卷积
  整个过程相当于在做稀疏化，但是预测能够通用，计算量和参数量都很节省
参考：
  https://blog.csdn.net/linmingan/article/details/79800956

4.3 CliqueNet
论文名称：Convolutional Neural Networks with Alternately Updated Clique
论文地址：1802.10419v3
论文简要笔记:
  基于densenet思想设计的cliquenet，目标是尽可能减少参数量
  主要改进是：
  1.设计clique block，分为多个stage，stage1中使用densenet，之后的stage逐个更新原有的层，关键是不同stage中的层复用参数
  2.多尺度特征输出，合并不同block然后进行分类
参考：
  http://tech.ifeng.com/a/20180523/45000747_0.shtml

4.4 Peleenet
论文名称：Pelee: A Real-Time Object Detection System on Mobile Devices
论文地址：1804.06882v1
论文简要笔记:
  在densenet基础上改进了一个结构，称为peleenet
  其主要是使用了stem块，以及双通路的dense layer
  计算量比mobilenet v1低，imagenet以及voc，coco效果都更好
参考：
  https://blog.csdn.net/fengzhongluoleidehua/article/details/80297168

5 移动端网络结构
5.1 Squeezenet
论文名称：SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND <0.5MB MODEL SIZE
论文地址：1602.07360v4
论文简要笔记:
  类似于AlexNet的堆叠卷积结构
  全部使用1*1卷积和3*3卷积
  其主体结构值得参考，可以使用更加复杂的block代替原有的堆叠卷积层
参考：
  https://blog.csdn.net/qinqbaobei/article/details/53946520
  https://blog.csdn.net/shenxiaolu1984/article/details/51444525

5.2 Mobilenet v1
论文名称：MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
论文地址：1704.04861v1
论文简要笔记:
  MobileNet，使用深度分离depthwise卷积和1*1卷积代替普通卷积，大幅度降低计算，保持原有性能不降低很多
  实验证明，计算量越大，性能一般可以更好，分辨率能够比模型深度取得更好的效果
参考：
  https://blog.csdn.net/jesse_mx/article/details/70766871
  https://blog.csdn.net/u013082989/article/details/77970196

5.3 Shufflenet
论文名称：ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices
论文地址：1707.01083v1
论文简要笔记:
  本文发现目前基于深度分离卷积的模型，例如ResNeXt和Xception中，一个问题就是1*1卷积计算和参数占比都很大
  为此本文提出使用group 1*1卷积，搭配shuffle操作，降低计算量
  shufflenet在小计算量模型和场景检测上比mobilenet方案有较大提升
参考：
  https://blog.csdn.net/dxmkkk/article/details/76797648?locationNum=10&fps=1
  https://blog.csdn.net/shuzfan/article/details/77141425

5.4 Mobilenet v2
论文名称：Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation
论文地址：1801.04381v2
论文简要笔记:
  使用倒残差结构，中间层维度大，两边
  每个块最后一个1*1卷积之后不跟上非线性变换
  所以是linearbottleneck维度小
参考：
  https://blog.csdn.net/u014380165/article/details/79200958
  https://blog.csdn.net/qq_14975217/article/details/79410324

6 Other/Mix
6.1 DPN
论文名称：
论文地址：（arXiv）https://https://arxiv.org/pdf/ID.pdf
论文简要笔记:
参考：（中文或英文Blog）

6.2 SENet
论文名称：
论文地址：（arXiv）https://https://arxiv.org/pdf/ID.pdf
论文简要笔记:
参考：（中文或英文Blog）

6.3 AutoML
论文名称：
论文地址：（arXiv）https://https://arxiv.org/pdf/ID.pdf
论文简要笔记:
参考：（中文或英文Blog）

7 Survey
7.1 Network in Network
论文名称：
论文地址：（arXiv）https://https://arxiv.org/pdf/ID.pdf
论文简要笔记:
参考：（中文或英文Blog）

7.2 Design Pattern
论文名称：
论文地址：（arXiv）https://https://arxiv.org/pdf/ID.pdf
论文简要笔记:
参考：（中文或英文Blog）

8 library
8.1 caffe cls
8.2 tensorflow slim
